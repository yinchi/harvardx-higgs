[["index.html", "HarvardX 125.9x Project: Higgs dataset classification Preface 0.1 R setup", " HarvardX 125.9x Project: Higgs dataset classification Yin-Chi Chan 2022-09-19 Preface This template generates output in both HTML and PDF form. Code chunks are typeset in Fira Code, with code ligatures enabled. 0.1 R setup # R 4.1 key features: new pipe operator, \\(x) as shortcut for function(x) # R 4.0 key features: stringsAsFactors = FALSE by default, raw character strings r&quot;()&quot; if (packageVersion(&#39;base&#39;) &lt; &#39;4.1.0&#39;) { stop(&#39;This code requires R &gt;= 4.1.0!&#39;) } if(!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) if(!require(&quot;keras&quot;)) { install.packages(&quot;keras&quot;) keras::install_keras() } library(pacman) p_load(data.table, dtplyr, tidyverse, R.utils, Rfast, knitr, lightgbm, keras, caret, pROC, knitr, conflicted) conflict_prefer(&#39;summarize&#39;, &#39;dplyr&#39;) conflict_prefer(&#39;summarise&#39;, &#39;dplyr&#39;) conflict_prefer(&#39;filter&#39;, &#39;dplyr&#39;) conflict_prefer(&#39;between&#39;, &#39;dplyr&#39;) conflict_prefer(&#39;auc&#39;, &#39;pROC&#39;) # Somehow, this seems to prevent memory leaks tensorflow::tf$compat$v1$disable_eager_execution() "],["introduction.html", "Chapter 1 Introduction 1.1 The HIGGS dataset 1.2 Creating the final test splits 1.3 Optimization metric", " Chapter 1 Introduction This report partially fufills the requirements for the HarvardX course PH125.9x: “Data Science: Capstone”. The objective of this project is to apply machine learning techniques beyond standard linear regression to a publicly available dataset of choice. 1.1 The HIGGS dataset The HIGGS dataset is a synthetic dataset simulating particle accelerator data (Baldi, Sadowski, and Whiteson 2014). Although the details of the simulated particle collisions are beyond the scope of this project, We summarize the contents of the dataset as follows. The HIGGS dataset is available from the UCI Machine Learning Repository. The following code loads the dataset, downloading and unzipping the CSV file as necessary: options(timeout=1800) # Give more time for the download to complete if(!dir.exists(&#39;data_raw&#39;)) { dir.create(&#39;data_raw&#39;) } if(!file.exists(&#39;data_raw/HIGGS.csv&#39;)) { download.file( &#39;https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz&#39;, &#39;data_raw/HIGGS.csv.gz&#39;) gunzip(&#39;data_raw/HIGGS.csv.gz&#39;) } # Load dataset into memory higgs_all &lt;- fread(&#39;data_raw/HIGGS.csv&#39;) # Assign column names (csv contains no headers) colnames(higgs_all) &lt;- c(&#39;signal&#39;, &#39;lepton_pT&#39;, &#39;lepton_eta&#39;, &#39;lepton_phi&#39;, &#39;missing_E_mag&#39;, &#39;missing_E_phi&#39;, &#39;jet1_pT&#39;, &#39;jet1_eta&#39;, &#39;jet1_phi&#39;, &#39;jet1_btag&#39;, &#39;jet2_pT&#39;, &#39;jet2_eta&#39;, &#39;jet2_phi&#39;, &#39;jet2_btag&#39;, &#39;jet3_pT&#39;, &#39;jet3_eta&#39;, &#39;jet3_phi&#39;, &#39;jet3_btag&#39;, &#39;jet4_pT&#39;, &#39;jet4_eta&#39;, &#39;jet4_phi&#39;, &#39;jet4_btag&#39;, &#39;m_jj&#39;, &#39;m_jjj&#39;, &#39;m_lv&#39;, &#39;m_jlv&#39;, &#39;m_bb&#39;, &#39;m_wbb&#39;, &#39;m_wwbb&#39;) # Separate input and output columns xAll &lt;- higgs_all %&gt;% select(-signal) %&gt;% as.data.table() yAll &lt;- higgs_all %&gt;% select(signal) %&gt;% as.data.table() rm(higgs_all) The HIGGS dataset contains 28 features and one binary target, signal. The signal value of an observation corresponds to whether a particle collision produced a Higgs boson as an intermediate product. Two possible processes are considered with the same input particles, one of which generates the Higgs boson (the “signal” process) and one which does not (the “background” process). Of the 28 features in the dataset, the last seven features, prefixed ‘m_’, are called “high-level” features and are based on computing the mass of expected intermediate decay products in the signal and background processes, assuming that the observed final decay products were generated by each process. The 21 “low-level” features consist of momentum data for a lepton and four jets, b-tags for each jet marking the likelihood that the jet is associated with a bottom quark, and partial information of “missing” total momentum caused by undetected decay products such as neutrinos. Due to the nature of the simulated particle colliders and detectors, full directional data of this missing momentum is not available. 1.2 Creating the final test splits Since Keras, one of the libraries we will use, uses “validation” internally when fitting a model, we will use the term “final validation set” to denote the final hold-out set for post-model evaluation. # Create 10% test set split set.seed(1) idx &lt;- createDataPartition(yAll$signal, p = 0.1, list = F) gc() x &lt;- xAll[-idx,] y &lt;- yAll[-idx,] xFinalTest &lt;- x[idx,] yFinalTest &lt;- y[idx,] rm(xAll, yAll) gc() 1.3 Optimization metric We will optimize the area under the receiver operating curve (AUC) of our models. The AUC is defined such that a perfect classifer has an AUC of 1 and a random classifier has an AUC of 0.5. An advantage of using the AUC is that it reflects the trade-off between the true and false positive rates of a model depending on the chosen classification threshold. We will use the pROC package to create and plot ROC curves, and to compute their area. References "],["data-analysis.html", "Chapter 2 Data analysis 2.1 Low-level momentum features 2.2 Angular features 2.3 \\(b\\)-tag features 2.4 High-level features 2.5 Final data transformation", " Chapter 2 Data analysis In this section, we analyze the various input features of the HIGGS dataset and apply deskewing and \\(z\\)-score normalization. 2.1 Low-level momentum features The input features containing _pT are related to transverse (perpendicular to the input beams) momentum, as is the high-level feature missing_E_mag. Histograms of these features are plotted as follows: x |&gt; select(c(contains(&#39;_pT&#39;), &#39;missing_E_mag&#39;)) |&gt; as.data.frame() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free&quot;, nrow = 2) + ggtitle(&#39;Momentum features&#39;) The momentum features are all quite skewed. To deskew the data, we apply a log transformation: x |&gt; select(c(contains(&#39;_pT&#39;), &#39;missing_E_mag&#39;)) |&gt; as.data.frame() |&gt; log() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free&quot;, nrow = 2) + ggtitle(&#39;Momentum features (log transform)&#39;) The skewness of the data before and after log transformation is as follows: tibble( Feature = x |&gt; select(c(contains(&#39;_pT&#39;), &#39;missing_E_mag&#39;)) |&gt; as.data.table() |&gt; colnames(), Skewness = x |&gt; select(c(contains(&#39;_pT&#39;), &#39;missing_E_mag&#39;)) |&gt; as.data.table() |&gt; as.matrix() |&gt; colskewness(), &#39;Skewness (log)&#39; = x |&gt; select(c(contains(&#39;_pT&#39;), &#39;missing_E_mag&#39;)) |&gt; as.data.table() |&gt; log() |&gt; as.matrix() |&gt; colskewness() ) |&gt; kable(align = &#39;lrr&#39;, booktabs = T, linesep = &#39;&#39;) Feature Skewness Skewness (log) lepton_pT 1.759399 0.1494493 jet1_pT 1.902766 0.1615350 jet2_pT 1.966270 0.0772273 jet3_pT 1.706359 0.0125490 jet4_pT 1.726289 0.2643133 missing_E_mag 1.487827 -0.8840618 2.2 Angular features The features containing _eta or _phi describe angular data of the detected products of the simulated collision processes. Histograms for these features are as follows: x |&gt; select(c(contains(&#39;_eta&#39;), contains(&#39;_phi&#39;))) |&gt; as.data.frame() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free_y&quot;, nrow = 3) + xlim(-pi,pi) + ggtitle(&#39;Angular features&#39;) The histograms above do not show significant skew; therefore, deskewing will not be applied to these input features. 2.3 \\(b\\)-tag features Each \\(b\\)-tag feature contains three possible values: x |&gt; select(contains(&#39;_btag&#39;)) |&gt; as.data.frame() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free&quot;, nrow = 2) + ggtitle(&#39;b-tag features&#39;) Interestingly, the \\(b\\)-tag data is encoded to have unit mean, but not unit standard deviation: # b-tag means tibble( Mean = x |&gt; select(contains(&#39;_btag&#39;)) |&gt; as.data.table() |&gt; as.matrix() |&gt; colMeans(), &#39;Standard Deviation&#39; = x |&gt; select(contains(&#39;_btag&#39;)) |&gt; as.data.table() |&gt; as.matrix() |&gt; colVars(std=T) ) |&gt; rownames_to_column(&#39;Feature&#39;) |&gt; kable(align = &#39;lrr&#39;, booktabs = T, linesep = &#39;&#39;) Feature Mean Standard Deviation 1 1.0000549 1.027791 2 1.0000157 1.049421 3 1.0000600 1.193689 4 0.9998079 1.400151 2.4 High-level features The high-level features for the HIGGS dataset are related to tranverse momentum and, as with the low-level momentum features, are quite skewed: x |&gt; select(contains(&#39;m_&#39;)) |&gt; as.data.frame() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free&quot;, nrow = 3) + ggtitle(&#39;High-level features&#39;) x |&gt; select(contains(&#39;m_&#39;)) |&gt; as.data.frame() |&gt; log() |&gt; gather() |&gt; ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = &quot;free&quot;, nrow = 3) + ggtitle(&#39;High-level features (log transform)&#39;) The skewness of the high-level features, before and after log transformation, are as follows: tibble( Feature = x |&gt; select(contains(&#39;m_&#39;)) |&gt; as.data.table() |&gt; colnames(), Skewness = x |&gt; select(contains(&#39;m_&#39;)) |&gt; as.data.table() |&gt; as.matrix() |&gt; colskewness(), &#39;Skewness (log)&#39; = x |&gt; select(contains(&#39;m_&#39;)) |&gt; as.data.table() |&gt; log() |&gt; as.matrix() |&gt; colskewness() ) |&gt; kable(align = &#39;lrr&#39;, booktabs = T, linesep = &#39;&#39;) Feature Skewness Skewness (log) m_jj 6.529976 1.1279300 m_jjj 5.007659 1.4937149 m_lv 4.614399 2.9117405 m_jlv 2.850652 0.7598249 m_bb 2.424460 -0.3608753 m_wbb 2.687129 0.8387949 m_wwbb 2.548881 1.0247968 2.5 Final data transformation The following code applies log transformation to selected columns of the HIGGS training dataset. We will also convert our data types into matrices here for input into subsequent stages of our analysis: # Apply log transform x &lt;- x |&gt; mutate(across( c(contains(&#39;m_&#39;), contains(&#39;_pT&#39;), contains(&#39;_mag&#39;)), log)) |&gt; as.data.table() # Convert to matrices x &lt;- x %&gt;% as.matrix() gc() The following code then scales the training data to have zero mean and unit standard deviation: m &lt;- colMeans(x) sd &lt;- colVars(x, std = T) # std=T -&gt; compute st. dev. instead of variance x &lt;- scale(x, center = m, scale = sd) Finally, we extract the target values to a vector: y &lt;- y$signal "],["gpu-accelerated-machine-learning-using-keras.html", "Chapter 3 GPU-accelerated machine learning using Keras 3.1 Tuning the learning rate 3.2 Effect of MLP depth and breadth 3.3 Dropping high-level features", " Chapter 3 GPU-accelerated machine learning using Keras We will use the keras package for machine learning. The keras package allows us to model the data using neural networks (NN); in this case, we will use a straightforward Multilayer Perceptron (MLP) with an Rectified Linear Unit (ReLU) activation function on all hidden nodes and a logistic activation function on the output node, thus giving a final estimate between 0 and 1 of the probability that a given observation corresponds to the “signal” process. The ReLu activation function, \\(\\mathrm{relu}(x) = \\max(0,x)\\), is chosen for its simplicity and computational efficiency. Note that in theory, an NN with sufficient breadth can approximate any function, even with just a single hidden layer (Asadi and Jiang 2020). However, we want a model that can learn the general characteristics of the signal and background processes from which our data is obtained. Therefore, to avoid overfitting, we withhold 20% of the training data in each epoch for validation. Additionally, 50% dropout is applied to each hidden layer, meaning that only half of the hidden nodes are used in each training batch. This has also been shown to reduce overfitting effectively (Srivastava et al. 2014). 3.1 Tuning the learning rate The following code plots the binary cross-entropy (a loss metric for binary classification networks) after each training epoch for three different learning rates, using the Adam(Kingma and Ba 2014) optimizer. We will train networks with 3 hidden layers of 256 nodes each. train_keras_history &lt;- function(x, y, depth, breadth, dropout = 0.5, learning_rate=0.0002, epochs = 50) { model &lt;- keras_model_sequential() model |&gt; layer_dense(breadth, &#39;relu&#39;, input_shape = ncol(x)) |&gt; layer_dropout(rate = dropout) # subsequent hidden layers if (depth &gt; 1) { for (layer in seq(2,depth)) { model |&gt; layer_dense(breadth, &#39;relu&#39;) |&gt; layer_dropout(rate = dropout) } } # output layer (logistic activation function for binary classification) model |&gt; layer_dense(1, &#39;sigmoid&#39;) # compile model model |&gt; keras::compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_adam(learning_rate = learning_rate), metrics = metric_auc() ) # a larger batch size trains faster but uses more GPU memory history &lt;- model |&gt; fit(x, y, epochs = epochs, batch_size = 8192, validation_split = 0.2) rm(model) gc() k_clear_session() tensorflow::tf$compat$v1$reset_default_graph() history } tensorflow::set_random_seed(42, disable_gpu = F) if (!file.exists(&#39;cache/nn_results.RDdata&#39;)) { history1 &lt;- train_keras_history(x, y, 3, 256, learning_rate = 1e-3) history2 &lt;- train_keras_history(x, y, 3, 256, learning_rate = 5e-4) history3 &lt;- train_keras_history(x, y, 3, 256, learning_rate = 2e-4) save(history1, history2, history3, file = &#39;cache/nn_results.RDdata&#39;) } load(&#39;cache/nn_results.RDdata&#39;) tensorflow::set_random_seed(42, disable_gpu = F) tibble( epoch = seq(50), `1e-3` = history1$metrics$val_loss, `5e-4` = history2$metrics$val_loss, `2e-4` = history3$metrics$val_loss) |&gt; pivot_longer(-epoch,&#39;lr&#39;,values_to = &#39;loss&#39;) |&gt; mutate(lr = as.numeric(lr)) |&gt; ggplot(aes(epoch,loss, color=as.factor(lr))) + geom_line() + xlab(&#39;Epochs&#39;) + ylab(&#39;Binary cross-entropy&#39;) + labs(color=&#39;Learning Rate&#39;) The results show that a smaller learning rate provides smoother loss as a function of the number of training epochs, but takes longer to train. Nevertheless, even the lowest learning rate shown above results in reasonably fast convergence. On the other hand, a learning rate that is too large may lead to non-convergence of the loss function. Based on the above results, we will use a training rate of \\(2\\times10^{-4}\\) for all subsequent modelling. 3.2 Effect of MLP depth and breadth The following function trains an MLP with the specified number of hidden layers (depth) and nodes per hidden layer (breadth), and returns the model and training history. We will use the Adam(Kingma and Ba 2014) optimizer with a learning rate of \\(2\\times 10^{-4}\\) and 50 epochs. # Tuning values given defaults are not included in our parameter search for this # project. train_keras_auc &lt;- function(x, y, depth, breadth, dropout = 0.5, learning_rate = 0.0002, epochs = 50) { cat(&#39;DNN: &#39;, depth, &#39;x&#39;, breadth, &#39;\\n&#39;) model &lt;- keras_model_sequential() # By default, Keras applies Glorot uniform initialization for weights # and zero initialization for biases. Glorot uniform initialization # samples weights from Uniform(-sqrt(6/n),sqrt(6/n)) where n is the # sum of in and out nodes between two input/hidden/output layers. # first hidden layer model |&gt; layer_dense(units = breadth, activation = &#39;relu&#39;, input_shape = ncol(x)) |&gt; layer_dropout(rate = dropout) # subsequent hidden layers if (depth &gt; 1) { for (layer in seq(2,depth)) { model |&gt; layer_dense(units = breadth, activation = &#39;relu&#39;) |&gt; layer_dropout(rate = dropout) } } # output layer (logistic activation function for binary classification) model |&gt; layer_dense(units = 1, activation = &#39;sigmoid&#39;) # compile model model |&gt; keras::compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_adam(learning_rate = learning_rate), metrics = metric_auc() ) # a larger batch size trains faster but uses more GPU memory history &lt;- model |&gt; fit(x, y, epochs = epochs, batch_size = 8192, validation_split = 0.2) ypred &lt;- model |&gt; predict(x, batch_size = 8192) |&gt; as.vector() auc &lt;- roc(y,ypred) |&gt; auc() |&gt; as.numeric() rm(model) gc() k_clear_session() tensorflow::tf$compat$v1$reset_default_graph() auc } The following code computes and plots the AUC for NNs with 1 to 5 hidden layers and from 16 to 2048 hidden nodes in each hidden layer: # Try NN training for different NN depths and breadths. Cache using .RData file. tensorflow::set_random_seed(42, disable_gpu = F) if (!file.exists(&#39;cache/nn_results2.RDdata&#39;)) { nn_results &lt;- tibble(depth = integer(), breadth = integer(), auc = numeric()) for(l in 1:5) { # depth: number of hidden layers for (n in 2^c(5:11)) { # breadth: hidden nodes per layer nn_results &lt;- nn_results |&gt; add_row(depth = l, breadth = n, auc = train_keras_auc(x, y, l, n)) } } save(nn_results, file = &#39;cache/nn_results2.RDdata&#39;) } load(&#39;cache/nn_results2.RDdata&#39;) tensorflow::set_random_seed(42, disable_gpu = F) # heatmap of AUC vs depth and breadth nn_results |&gt; ggplot(aes(as.factor(depth), as.factor(breadth), fill = auc)) + geom_tile() + geom_text(aes(label = round(auc,4)), color = &quot;black&quot;, size = 3) + scale_fill_viridis_c() + xlab(&#39;Depth (number of hidden layers)&#39;) + ylab(&#39;Breadth (hidden nodes per layer)&#39;) The results show that for NNs with at least 64 nodes per hidden layer, there is little difference between having three layers or more; however, there is a slight advantage to having three hidden layers compared to two. In all cases, the widest NN at each depth provides the best AUC. Therefore, the 3x2048 NN appears to provide the best trade-off between network complexity and AUC. If memory or compute resources are limited, a 2x512 NN also appears to give reasonable estimation power. 3.3 Dropping high-level features Since the high-level features in the HIGGS dataset are derived from the other, low-level features, it is clear that a good model should be able to learn the data without use of these features. The following code trains a 3x2048 NN on the HIGGS dataset once using all features and once just the low-level features: tensorflow::set_random_seed(42, disable_gpu = F) if (!file.exists(&#39;cache/nn_results3.RDdata&#39;)) { auc_low_only &lt;- train_keras_auc(x[,1:21], y, 3, 2048, learning_rate = 2e-4) save(auc_low_only, file=&#39;cache/nn_results3.RDdata&#39;) } load(&#39;cache/nn_results3.RDdata&#39;) tensorflow::set_random_seed(42, disable_gpu = F) The following code displays the AUC for the both models: tibble( Features = c(&#39;All&#39;, &#39;Low-level only&#39;), AUC = c( nn_results |&gt; filter(breadth == 2048 &amp; depth == 3) |&gt; pull(auc), auc_low_only ) ) |&gt; kable(align = &#39;lr&#39;, booktabs = T, linesep = &#39;&#39;) Features AUC All 0.8766273 Low-level only 0.8663024 The AUC for the low-level-only NN is only slightly lower than that for the full network, demonstrating that the NN is able to learn the complex non-linear relationships between the independent and dependent variables on it its own without the need for additional derived variables. References "],["final-model-selection-and-validation.html", "Chapter 4 Final model selection and validation", " Chapter 4 Final model selection and validation "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
